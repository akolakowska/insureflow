{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "model = 'azure_openai'\n",
    "\n",
    "# for Azure OpenAI model\n",
    "api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "gpt_api_version = os.getenv('AZURE_GPT_API_VERSION')\n",
    "embedding_api_version = os.getenv('AZURE_EMBEDDING_API_VERSION')\n",
    "\n",
    "if(model == 'local'):\n",
    "    Settings.embed_model = embed_model\n",
    "    Settings.llm = llm\n",
    "elif(model == 'azure_openai'):\n",
    "    llm = AzureOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        deployment_name=\"gpt-4o\",\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=gpt_api_version,\n",
    "        temperature=0.1,  # Lower temperature for more consistent decision-making\n",
    "        timeout=60,  # Increased timeout value\n",
    "    )\n",
    "    embed_model = AzureOpenAIEmbedding(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment_name=\"text-embedding-ada-002\",\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=embedding_api_version,\n",
    "    )\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "elif(model == 'openai'):\n",
    "    llm = OpenAI(model=\"gpt-4o\")\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = OpenAIEmbedding(\n",
    "        model=\"text-embedding-3-small\", embed_batch_size=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.agent.workflow import FunctionAgent, AgentWorkflow\n",
    "from llama_index.core.workflow import Context, InputRequiredEvent, HumanResponseEvent\n",
    "#from llama_index.core.events import InputRequiredEvent, HumanResponseEvent\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"RankRejectAgent\")\n",
    "\n",
    "# Define tools for the agents\n",
    "async def evaluate_hazard_classification(ctx: Context, property_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates the hazard classification of a property based on building type, \n",
    "    construction materials, and occupancy.\n",
    "    \n",
    "    Args:\n",
    "        property_data: Dictionary containing property details\n",
    "        \n",
    "    Returns:\n",
    "        A description of the hazard assessment with score (1-5)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting hazard classification assessment\")\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    \n",
    "    # Simulate hazard classification logic\n",
    "    building_type = property_data.get(\"building_type\", \"\")\n",
    "    construction = property_data.get(\"construction\", \"\")\n",
    "    occupancy = property_data.get(\"occupancy\", \"\")\n",
    "    \n",
    "    logger.info(f\"Analyzing property: {building_type} building with {construction} construction\")\n",
    "    \n",
    "    # Sample logic - would be more sophisticated in production\n",
    "    hazard_factors = {\n",
    "        \"building_type\": {\n",
    "            \"Office\": 1,\n",
    "            \"Retail\": 2,\n",
    "            \"Manufacturing\": 3,\n",
    "            \"Warehouse\": 3,\n",
    "            \"Heavy Industrial\": 4,\n",
    "            \"Nightclub\": 5\n",
    "        },\n",
    "        \"construction\": {\n",
    "            \"Concrete\": 1,\n",
    "            \"Steel Frame\": 2,\n",
    "            \"Brick\": 2,\n",
    "            \"Wood Frame\": 4,\n",
    "            \"Mixed\": 3\n",
    "        }\n",
    "    }\n",
    "\n",
    "    bt_score = hazard_factors[\"building_type\"].get(building_type, 3)\n",
    "    const_score = 3  # Default score\n",
    "    for material, score in hazard_factors[\"construction\"].items():\n",
    "        if material.lower() in construction.lower():\n",
    "            const_score = score\n",
    "            break\n",
    "\n",
    "     # Calculate overall hazard score\n",
    "    hazard_score = (bt_score + const_score) / 2\n",
    "    logger.info(f\"Calculated hazard score: {hazard_score}\")\n",
    "    \n",
    "    # Store the result in context\n",
    "    if \"assessment_results\" not in current_state:\n",
    "        current_state[\"assessment_results\"] = {}\n",
    "    \n",
    "    current_state[\"assessment_results\"][\"hazard\"] = {\n",
    "        \"score\": hazard_score,\n",
    "        \"building_type_assessment\": f\"{building_type}: Risk level {bt_score}\",\n",
    "        \"construction_assessment\": f\"{construction}: Risk level {const_score}\",\n",
    "        \"occupancy_details\": occupancy\n",
    "    }\n",
    "    \n",
    "    # FIXED: Track completion as a list instead of a set for better serialization\n",
    "    if \"completed_assessments\" not in current_state:\n",
    "        current_state[\"completed_assessments\"] = []\n",
    "        \n",
    "    if \"hazard\" not in current_state[\"completed_assessments\"]:\n",
    "        current_state[\"completed_assessments\"].append(\"hazard\")\n",
    "    \n",
    "    logger.info(f\"Updated completed_assessments: {current_state['completed_assessments']}\")\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    logger.info(\"Hazard assessment completed and stored in context\")\n",
    "    return f\"Hazard assessment completed. Overall hazard score: {hazard_score:.1f}/5.0\"\n",
    "\n",
    "async def evaluate_vulnerability(ctx: Context, security_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates the vulnerability of a property based on security systems,\n",
    "    protective measures, and other safety features.\n",
    "    \n",
    "    Args:\n",
    "        security_data: Dictionary containing security details\n",
    "        \n",
    "    Returns:\n",
    "        A description of the vulnerability assessment with score (1-5)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting vulnerability assessment\")\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    \n",
    "    # Simulate vulnerability assessment\n",
    "    has_sprinklers = security_data.get(\"sprinklers\", False)\n",
    "    alarm_system = security_data.get(\"alarm_system\", \"None\")\n",
    "    \n",
    "    logger.info(f\"Security features: Sprinklers={has_sprinklers}, Alarm={alarm_system}\")\n",
    "    \n",
    "    # Basic scoring\n",
    "    sprinkler_score = 1 if has_sprinklers else 4\n",
    "    \n",
    "    alarm_scores = {\n",
    "        \"None\": 5,\n",
    "        \"Local\": 3,\n",
    "        \"Monitored\": 2,\n",
    "        \"Grade A - 24hr Monitored\": 1\n",
    "    }\n",
    "    alarm_score = alarm_scores.get(alarm_system, 3)\n",
    "    \n",
    "    # Calculate overall vulnerability score\n",
    "    vulnerability_score = (sprinkler_score + alarm_score) / 2\n",
    "    logger.info(f\"Calculated vulnerability score: {vulnerability_score}\")\n",
    "\n",
    "    # Store the result\n",
    "    if \"assessment_results\" not in current_state:\n",
    "        current_state[\"assessment_results\"] = {}\n",
    "    \n",
    "    current_state[\"assessment_results\"][\"vulnerability\"] = {\n",
    "        \"score\": vulnerability_score,\n",
    "        \"sprinkler_assessment\": f\"Sprinklers: {'Present' if has_sprinklers else 'Absent'}, Risk level {sprinkler_score}\",\n",
    "        \"alarm_assessment\": f\"Alarm: {alarm_system}, Risk level {alarm_score}\"\n",
    "    }\n",
    "    \n",
    "    # FIXED: Track completion as a list instead of a set for better serialization\n",
    "    if \"completed_assessments\" not in current_state:\n",
    "        current_state[\"completed_assessments\"] = []\n",
    "        \n",
    "    if \"vulnerability\" not in current_state[\"completed_assessments\"]:\n",
    "        current_state[\"completed_assessments\"].append(\"vulnerability\")\n",
    "    \n",
    "    logger.info(f\"Updated completed_assessments: {current_state['completed_assessments']}\")\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    logger.info(\"Vulnerability assessment completed and stored in context\")\n",
    "    return f\"Vulnerability assessment completed. Overall vulnerability score: {vulnerability_score:.1f}/5.0\"\n",
    "\n",
    "async def evaluate_cat_modeling(ctx: Context, location_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates the catastrophe risk based on location, flood zones, \n",
    "    earthquake potential, etc.\n",
    "    \n",
    "    Args:\n",
    "        location_data: Dictionary containing location details\n",
    "        \n",
    "    Returns:\n",
    "        A description of the CAT modeling assessment with score (1-5)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting CAT modeling assessment\")\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    \n",
    "    # In production, this would call specialized CAT modeling services\n",
    "    address = location_data.get(\"address\", \"\")\n",
    "    logger.info(f\"Analyzing location: {address}\")\n",
    "    \n",
    "    # Mock implementation - would use real data in production\n",
    "    # Simulating a flood zone and earthquake risk evaluation\n",
    "    if \"flood\" in address.lower() or \"coastal\" in address.lower():\n",
    "        flood_risk = 4.5\n",
    "    else:\n",
    "        flood_risk = 2.0\n",
    "    \n",
    "    # Simple geographic rules - would use proper services in production\n",
    "    geo_mapping = {\n",
    "        \"california\": 4.5,  # High earthquake risk\n",
    "        \"florida\": 4.0,     # Hurricane risk\n",
    "        \"texas\": 3.5,       # Multiple hazards\n",
    "        \"new york\": 3.0,\n",
    "        \"london\": 2.0,\n",
    "        \"birmingham\": 1.5,\n",
    "        \"manchester\": 2.0\n",
    "    }\n",
    "    \n",
    "    earthquake_risk = 1.0  # Default low risk\n",
    "    for region, risk in geo_mapping.items():\n",
    "        if region.lower() in address.lower():\n",
    "            earthquake_risk = risk\n",
    "            break\n",
    "    \n",
    "    # Calculate overall CAT score\n",
    "    cat_score = max(flood_risk, earthquake_risk)  # Taking worst-case scenario\n",
    "    logger.info(f\"Calculated CAT score: {cat_score} (flood_risk: {flood_risk}, earthquake_risk: {earthquake_risk})\")\n",
    "    \n",
    "    # Store the result\n",
    "    if \"assessment_results\" not in current_state:\n",
    "        current_state[\"assessment_results\"] = {}\n",
    "    \n",
    "    current_state[\"assessment_results\"][\"cat_modeling\"] = {\n",
    "        \"score\": cat_score,\n",
    "        \"flood_risk_assessment\": f\"Flood risk level: {flood_risk:.1f}/5.0\",\n",
    "        \"earthquake_risk_assessment\": f\"Earthquake/natural disaster risk: {earthquake_risk:.1f}/5.0\",\n",
    "        \"location_analyzed\": address\n",
    "    }\n",
    "    \n",
    "    # FIXED: Track completion as a list instead of a set for better serialization\n",
    "    if \"completed_assessments\" not in current_state:\n",
    "        current_state[\"completed_assessments\"] = []\n",
    "        \n",
    "    if \"cat_modeling\" not in current_state[\"completed_assessments\"]:\n",
    "        current_state[\"completed_assessments\"].append(\"cat_modeling\")\n",
    "    \n",
    "    logger.info(f\"Updated completed_assessments: {current_state['completed_assessments']}\")\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    logger.info(\"CAT modeling assessment completed and stored in context\")\n",
    "    return f\"CAT modeling assessment completed. Overall CAT risk score: {cat_score:.1f}/5.0\"\n",
    "\n",
    "async def check_guidelines_compliance(ctx: Context, coverage_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Checks if the submission complies with underwriting guidelines for \n",
    "    limits, deductibles, and pricing factors.\n",
    "    \n",
    "    Args:\n",
    "        coverage_data: Dictionary containing coverage details\n",
    "        \n",
    "    Returns:\n",
    "        A description of the compliance assessment\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting guidelines compliance assessment\")\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    \n",
    "    # Mock guideline rules - would be loaded from database in production\n",
    "    guidelines = {\n",
    "        \"building_value\": {\n",
    "            \"min\": 500000,\n",
    "            \"max\": 10000000\n",
    "        },\n",
    "        \"deductible\": {\n",
    "            \"min_percentage\": 0.005,  # Minimum deductible should be 0.5% of building value\n",
    "            \"preferred_percentage\": 0.01  # Preferred deductible is 1% of building value\n",
    "        },\n",
    "        \"business_interruption\": {\n",
    "            \"max_ratio\": 2.0  # BI shouldn't be more than 2x building value\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Extract values\n",
    "    building_value = coverage_data.get(\"building_value\", 0)\n",
    "    deductible = coverage_data.get(\"deductible\", 0)\n",
    "    business_interruption = coverage_data.get(\"business_interruption\", 0)\n",
    "    \n",
    "    logger.info(f\"Analyzing coverage: Building value={building_value}, Deductible={deductible}, Business interruption={business_interruption}\")\n",
    "    \n",
    "    # Check compliance\n",
    "    compliance_issues = []\n",
    "    \n",
    "    if building_value < guidelines[\"building_value\"][\"min\"]:\n",
    "        compliance_issues.append(f\"Building value too low: £{building_value:,.2f} (minimum: £{guidelines['building_value']['min']:,.2f})\")\n",
    "    \n",
    "    if building_value > guidelines[\"building_value\"][\"max\"]:\n",
    "        compliance_issues.append(f\"Building value exceeds guideline maximum: £{building_value:,.2f} (maximum: £{guidelines['building_value']['max']:,.2f})\")\n",
    "    \n",
    "    min_required_deductible = building_value * guidelines[\"deductible\"][\"min_percentage\"]\n",
    "    if deductible < min_required_deductible:\n",
    "        compliance_issues.append(f\"Deductible too low: £{deductible:,.2f} (minimum required: £{min_required_deductible:,.2f})\")\n",
    "    \n",
    "    if business_interruption > building_value * guidelines[\"business_interruption\"][\"max_ratio\"]:\n",
    "        compliance_issues.append(f\"Business interruption coverage exceeds guidelines: £{business_interruption:,.2f} (maximum allowed: £{building_value * guidelines['business_interruption']['max_ratio']:,.2f})\")\n",
    "    \n",
    "    # Calculate compliance score - lower is better\n",
    "    if len(compliance_issues) == 0:\n",
    "        compliance_score = 1.0\n",
    "    else:\n",
    "        compliance_score = 1.0 + len(compliance_issues)\n",
    "    \n",
    "    logger.info(f\"Compliance assessment - Issues: {len(compliance_issues)}, Score: {compliance_score}\")\n",
    "    \n",
    "    # Store the result\n",
    "    if \"assessment_results\" not in current_state:\n",
    "        current_state[\"assessment_results\"] = {}\n",
    "    \n",
    "    current_state[\"assessment_results\"][\"guidelines_compliance\"] = {\n",
    "        \"score\": compliance_score,\n",
    "        \"compliant\": len(compliance_issues) == 0,\n",
    "        \"issues\": compliance_issues,\n",
    "        \"coverage_reviewed\": {\n",
    "            \"building_value\": building_value,\n",
    "            \"deductible\": deductible,\n",
    "            \"business_interruption\": business_interruption\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # FIXED: Track completion as a list instead of a set for better serialization\n",
    "    if \"completed_assessments\" not in current_state:\n",
    "        current_state[\"completed_assessments\"] = []\n",
    "        \n",
    "    if \"guidelines_compliance\" not in current_state[\"completed_assessments\"]:\n",
    "        current_state[\"completed_assessments\"].append(\"guidelines_compliance\")\n",
    "    \n",
    "    logger.info(f\"Updated completed_assessments: {current_state['completed_assessments']}\")\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    logger.info(\"Guidelines compliance assessment completed and stored in context\")\n",
    "    if len(compliance_issues) == 0:\n",
    "        return \"Guidelines compliance check passed. No issues found.\"\n",
    "    else:\n",
    "        return f\"Guidelines compliance check completed with {len(compliance_issues)} issue(s): \" + \"; \".join(compliance_issues)\n",
    "    \n",
    "async def make_decision(ctx: Context) -> str:\n",
    "    \"\"\"\n",
    "    Checks if all assessments are complete, then analyzes results and makes a final decision.\n",
    "    If confidence is low, requests human review.\n",
    "    \n",
    "    Returns:\n",
    "        Decision outcome with explanation\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting decision making process\")\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    \n",
    "    # FIXED: Check if all assessments are complete using a list instead of a set\n",
    "    completed_assessments = current_state.get(\"completed_assessments\", [])\n",
    "    required_assessments = [\"hazard\", \"vulnerability\", \"cat_modeling\", \"guidelines_compliance\"]\n",
    "    \n",
    "    missing_assessments = [a for a in required_assessments if a not in completed_assessments]\n",
    "    \n",
    "    if missing_assessments:\n",
    "        logger.info(f\"Cannot make decision yet. Missing assessments: {missing_assessments}\")\n",
    "        return f\"Cannot make decision yet. The following assessments are still pending: {', '.join(missing_assessments)}\"\n",
    "    \n",
    "    # Check if we already have a final decision (to prevent infinite loops)\n",
    "    if current_state.get(\"decision\", {}).get(\"final\", False):\n",
    "        decision = current_state.get(\"decision\", {}).get(\"outcome\", \"UNKNOWN\")\n",
    "        reason = current_state.get(\"decision\", {}).get(\"reason\", \"No reason provided\")\n",
    "        confidence = current_state.get(\"decision\", {}).get(\"confidence\", 0.0)\n",
    "        \n",
    "        decision_messages = {\n",
    "            \"PROCEED_TO_QUOTATION\": \"Submission approved to proceed to quotation.\",\n",
    "            \"RECOMMEND_SURVEYOR\": \"Recommend surveyor assessment before proceeding.\",\n",
    "            \"REJECT\": \"Submission rejected.\"\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Using existing final decision: {decision}\")\n",
    "        return f\"Final decision already made: {decision_messages.get(decision, decision)}. Reason: {reason}. Confidence: {confidence:.2f}\"\n",
    "    \n",
    "    # Continue with decision making if all assessments are complete\n",
    "    logger.info(\"All assessments are complete. Proceeding with decision making.\")\n",
    "    assessment_results = current_state.get(\"assessment_results\", {})\n",
    "    submission = current_state.get(\"submission_data\", {})\n",
    "    \n",
    "    # Gather scores from assessments\n",
    "    hazard_score = assessment_results.get(\"hazard\", {}).get(\"score\", 3.0)\n",
    "    vulnerability_score = assessment_results.get(\"vulnerability\", {}).get(\"score\", 3.0)\n",
    "    cat_score = assessment_results.get(\"cat_modeling\", {}).get(\"score\", 3.0)\n",
    "    compliance_score = assessment_results.get(\"guidelines_compliance\", {}).get(\"score\", 3.0)\n",
    "    is_compliant = assessment_results.get(\"guidelines_compliance\", {}).get(\"compliant\", False)\n",
    "    \n",
    "    logger.info(f\"Assessment scores - Hazard: {hazard_score}, Vulnerability: {vulnerability_score}, CAT: {cat_score}, Compliance: {compliance_score}\")\n",
    "    \n",
    "    # Calculate composite risk score (weighted average)\n",
    "    weights = {\n",
    "        \"hazard\": 0.3,\n",
    "        \"vulnerability\": 0.2,\n",
    "        \"cat\": 0.3,\n",
    "        \"compliance\": 0.2\n",
    "    }\n",
    "    \n",
    "    composite_score = (\n",
    "        hazard_score * weights[\"hazard\"] +\n",
    "        vulnerability_score * weights[\"vulnerability\"] +\n",
    "        cat_score * weights[\"cat\"] +\n",
    "        compliance_score * weights[\"compliance\"]\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Calculated composite risk score: {composite_score}\")\n",
    "    \n",
    "    # Decision logic\n",
    "    if not is_compliant:\n",
    "        decision = \"REJECT\"\n",
    "        reason = \"Submission does not comply with underwriting guidelines\"\n",
    "        confidence = 0.95\n",
    "    elif composite_score <= 2.5:\n",
    "        decision = \"PROCEED_TO_QUOTATION\"\n",
    "        reason = \"Risk profile is within acceptable parameters\"\n",
    "        confidence = min(1.0, max(0.0, 1.0 - (composite_score / 5.0)))\n",
    "    elif composite_score <= 3.5:\n",
    "        decision = \"RECOMMEND_SURVEYOR\"\n",
    "        reason = \"Risk profile requires additional assessment\"\n",
    "        confidence = 0.8\n",
    "    else:\n",
    "        decision = \"REJECT\"\n",
    "        reason = \"Risk profile exceeds acceptable parameters\"\n",
    "        confidence = min(1.0, max(0.0, composite_score / 5.0 - 0.2))\n",
    "    \n",
    "    logger.info(f\"Initial decision: {decision}, Reason: {reason}, Confidence: {confidence:.2f}\")\n",
    "    \n",
    "    # Human-in-the-loop for decisions with low confidence or rejections\n",
    "    if confidence < 0.7 or decision == \"REJECT\":\n",
    "        # Check if we already received human feedback (to prevent loops)\n",
    "        if current_state.get(\"human_feedback_received\", False):\n",
    "            logger.info(\"Human feedback was already received, using it to finalize decision\")\n",
    "            feedback_comment = current_state.get(\"human_feedback_comment\", \"No feedback provided\")\n",
    "        else:\n",
    "            logger.info(f\"Decision requires human review. Confidence: {confidence:.2f}\")\n",
    "            \n",
    "            # Request human feedback\n",
    "            feedback_comment = await request_human_feedback(ctx, \n",
    "                f\"Please review decision: {decision} for submission {submission.get('submission_id', 'Unknown')}. Confidence: {confidence:.2f}\")\n",
    "            \n",
    "            # Store the feedback\n",
    "            current_state[\"human_feedback_received\"] = True\n",
    "            current_state[\"human_feedback_comment\"] = feedback_comment\n",
    "            \n",
    "            await ctx.set(\"state\", current_state)\n",
    "            logger.info(f\"Human feedback received: {feedback_comment}\")\n",
    "        \n",
    "        # Modify the decision based on feedback if needed\n",
    "        if \"approve\" in feedback_comment.lower() or \"proceed\" in feedback_comment.lower():\n",
    "            # Override the decision if human approves\n",
    "            if decision == \"REJECT\":\n",
    "                decision = \"PROCEED_TO_QUOTATION\" \n",
    "                reason = f\"Risk profile approved by human reviewer despite system assessment\"\n",
    "                confidence = 0.85  # Human override increases confidence\n",
    "                logger.info(\"Decision changed to PROCEED_TO_QUOTATION based on human feedback\")\n",
    "        elif \"surveyor\" in feedback_comment.lower() or \"survey\" in feedback_comment.lower():\n",
    "            # Change to surveyor recommendation if that's what human suggests\n",
    "            if decision != \"RECOMMEND_SURVEYOR\":\n",
    "                decision = \"RECOMMEND_SURVEYOR\"\n",
    "                reason = f\"Human reviewer recommended additional assessment\"\n",
    "                confidence = 0.9  # Human override increases confidence\n",
    "                logger.info(\"Decision changed to RECOMMEND_SURVEYOR based on human feedback\")\n",
    "        elif \"reject\" in feedback_comment.lower() or \"decline\" in feedback_comment.lower():\n",
    "            # Confirm rejection if human agrees\n",
    "            if decision != \"REJECT\":\n",
    "                decision = \"REJECT\"\n",
    "                reason = f\"Human reviewer recommended rejection\"\n",
    "                confidence = 0.95  # Human override increases confidence\n",
    "                logger.info(\"Decision changed to REJECT based on human feedback\")\n",
    "    \n",
    "    # Store the final decision\n",
    "    current_state[\"decision\"] = {\n",
    "        \"outcome\": decision,\n",
    "        \"reason\": reason,\n",
    "        \"composite_score\": composite_score,\n",
    "        \"confidence\": confidence,\n",
    "        \"requires_human_review\": confidence < 0.8,\n",
    "        \"human_reviewed\": current_state.get(\"human_feedback_received\", False),\n",
    "        \"assessment_summary\": {\n",
    "            \"hazard_score\": hazard_score,\n",
    "            \"vulnerability_score\": vulnerability_score,\n",
    "            \"cat_score\": cat_score,\n",
    "            \"compliance_score\": compliance_score,\n",
    "            \"is_compliant\": is_compliant\n",
    "        },\n",
    "        \"final\": True  # Mark this decision as final to prevent loops\n",
    "    }\n",
    "    \n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    decision_messages = {\n",
    "        \"PROCEED_TO_QUOTATION\": \"Submission approved to proceed to quotation.\",\n",
    "        \"RECOMMEND_SURVEYOR\": \"Recommend surveyor assessment before proceeding.\",\n",
    "        \"REJECT\": \"Submission rejected.\"\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Decision made and stored in context\")\n",
    "    return f\"Decision: {decision_messages[decision]} Reason: {reason}. Confidence: {confidence:.2f}\"\n",
    "\n",
    "\n",
    "async def send_notification(ctx: Context) -> str:\n",
    "    \"\"\"\n",
    "    Formats and sends notification email based on the decision.\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation of notification sent\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting notification preparation\")\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    decision = current_state.get(\"decision\", {})\n",
    "\n",
    "    # Check if this is a final decision\n",
    "    if not decision.get(\"final\", False):\n",
    "        return \"Cannot send notification for a non-final decision. Decision requires human review first.\"\n",
    "\n",
    "    # Get the decision outcome and reason\n",
    "    submission = current_state.get(\"submission_data\", {})\n",
    "    \n",
    "    # In production, this would use Azure Communication Services or similar\n",
    "    \n",
    "    # Format email content based on decision outcome\n",
    "    outcome = decision.get(\"outcome\", \"UNKNOWN\")\n",
    "    reason = decision.get(\"reason\", \"No reason provided\")\n",
    "    \n",
    "    logger.info(f\"Preparing notification for decision: {outcome}\")\n",
    "    \n",
    "    # Basic email templates\n",
    "    email_templates = {\n",
    "        \"PROCEED_TO_QUOTATION\": \"\"\"\n",
    "Subject: Submission {submission_id} Approved for Quotation\n",
    "\n",
    "Dear Distribution Team,\n",
    "\n",
    "The submission for {insured_name} (ID: {submission_id}) has been reviewed and approved to proceed to quotation.\n",
    "\n",
    "Risk Assessment Summary:\n",
    "- Hazard Score: {hazard_score}/5.0\n",
    "- Vulnerability Score: {vulnerability_score}/5.0\n",
    "- CAT Risk Score: {cat_score}/5.0\n",
    "- Compliance: {compliance}\n",
    "\n",
    "Decision: Proceed to Quotation\n",
    "Confidence: {confidence:.0%}\n",
    "\n",
    "Please proceed with the quotation process.\n",
    "\n",
    "Regards,\n",
    "Underwriting AI Assistant\n",
    "\"\"\",\n",
    "        \"RECOMMEND_SURVEYOR\": \"\"\"\n",
    "Subject: Submission {submission_id} Requires Surveyor Assessment\n",
    "\n",
    "Dear Distribution Team,\n",
    "\n",
    "The submission for {insured_name} (ID: {submission_id}) has been reviewed and requires a surveyor assessment before proceeding.\n",
    "\n",
    "Risk Assessment Summary:\n",
    "- Hazard Score: {hazard_score}/5.0\n",
    "- Vulnerability Score: {vulnerability_score}/5.0\n",
    "- CAT Risk Score: {cat_score}/5.0\n",
    "- Compliance: {compliance}\n",
    "\n",
    "Reason for surveyor recommendation: {reason}\n",
    "Confidence: {confidence:.0%}\n",
    "\n",
    "Please arrange for a risk assessment survey.\n",
    "\n",
    "Regards,\n",
    "Underwriting AI Assistant\n",
    "\"\"\",\n",
    "        \"REJECT\": \"\"\"\n",
    "Subject: Submission {submission_id} Rejected\n",
    "\n",
    "Dear Distribution Team,\n",
    "\n",
    "The submission for {insured_name} (ID: {submission_id}) has been reviewed and cannot proceed.\n",
    "\n",
    "Risk Assessment Summary:\n",
    "- Hazard Score: {hazard_score}/5.0\n",
    "- Vulnerability Score: {vulnerability_score}/5.0\n",
    "- CAT Risk Score: {cat_score}/5.0\n",
    "- Compliance: {compliance}\n",
    "\n",
    "Reason for rejection: {reason}\n",
    "Confidence: {confidence:.0%}\n",
    "\n",
    "If you have additional information that might change this assessment, please provide it.\n",
    "\n",
    "Regards,\n",
    "Underwriting AI Assistant\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Get template for current decision\n",
    "    template = email_templates.get(outcome, \"Unknown decision type\")\n",
    "    \n",
    "    # Fill in template variables\n",
    "    assessment_summary = decision.get(\"assessment_summary\", {})\n",
    "    email_content = template.format(\n",
    "        submission_id=submission.get(\"submission_id\", \"Unknown\"),\n",
    "        insured_name=submission.get(\"insured_name\", \"Unknown\"),\n",
    "        hazard_score=assessment_summary.get(\"hazard_score\", 0),\n",
    "        vulnerability_score=assessment_summary.get(\"vulnerability_score\", 0),\n",
    "        cat_score=assessment_summary.get(\"cat_score\", 0),\n",
    "        compliance=\"Compliant\" if assessment_summary.get(\"is_compliant\", False) else \"Non-compliant\",\n",
    "        reason=reason,\n",
    "        confidence=decision.get(\"confidence\", 0.0)\n",
    "    )\n",
    "    \n",
    "    # Store the notification in the state\n",
    "    current_state[\"notification\"] = {\n",
    "        \"recipient\": \"distribution_team@company.com\",\n",
    "        \"content\": email_content,\n",
    "        \"sent\": True,\n",
    "        \"timestamp\": \"2025-04-17T10:30:00Z\"  # This would be actual timestamp in production\n",
    "    }\n",
    "    \n",
    "    # Mark workflow as complete\n",
    "    current_state[\"workflow_complete\"] = True\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    # In production, this would actually send the email\n",
    "    logger.info(f\"Email notification prepared for distribution team regarding submission {submission.get('submission_id', 'Unknown')}\")\n",
    "    \n",
    "    return f\"Notification email prepared and ready to send for decision: {outcome}\"\n",
    "\n",
    "async def request_human_feedback(ctx: Context, question: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Requests human feedback using LlamaIndex's event system.\n",
    "    \n",
    "    Args:\n",
    "        ctx: Context object\n",
    "        question: Question to ask the human reviewer\n",
    "        \n",
    "    Returns:\n",
    "        Human feedback response\n",
    "    \"\"\"\n",
    "    logger.info(f\"Requesting human feedback: {question}\")\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    \n",
    "    # Get decision and submission details for context\n",
    "    decision = current_state.get(\"decision\", {})\n",
    "    submission = current_state.get(\"submission_data\", {})\n",
    "    assessment_results = current_state.get(\"assessment_results\", {})\n",
    "    \n",
    "    # Create a detailed prompt for the human reviewer\n",
    "    detailed_question = f\"\"\"\n",
    "HUMAN REVIEW REQUIRED:\n",
    "{question}\n",
    "\n",
    "Submission: {submission.get('submission_id')} - {submission.get('insured_name')}\n",
    "Property: {submission.get('property_details', {}).get('building_type')} at {submission.get('property_details', {}).get('address')}\n",
    "\n",
    "Assessment Results:\n",
    "- Hazard: {assessment_results.get('hazard', {}).get('score', 0)}/5.0\n",
    "- Vulnerability: {assessment_results.get('vulnerability', {}).get('score', 0)}/5.0\n",
    "- CAT Risk: {assessment_results.get('cat_modeling', {}).get('score', 0)}/5.0\n",
    "- Compliance: {'Compliant' if assessment_results.get('guidelines_compliance', {}).get('compliant', False) else 'Non-compliant'}\n",
    "\n",
    "System Decision: {decision.get('outcome', 'Unknown')}\n",
    "Reason: {decision.get('reason', 'No reason provided')}\n",
    "Confidence: {decision.get('confidence', 0):.0%}\n",
    "\n",
    "Please review and provide feedback. Type 'approve' to confirm the decision, or provide specific guidance:\n",
    "\"\"\"\n",
    "    \n",
    "    # Use LlamaIndex's wait_for_event to get human input\n",
    "    response = await ctx.wait_for_event(\n",
    "        HumanResponseEvent,\n",
    "        waiter_id=question,\n",
    "        waiter_event=InputRequiredEvent(\n",
    "            prefix=detailed_question,\n",
    "            user_name=\"Underwriter\",\n",
    "        ),\n",
    "        requirements={\"user_name\": \"Underwriter\"},\n",
    "    )\n",
    "    \n",
    "    # Record the feedback in the state\n",
    "    if \"human_feedback\" not in current_state:\n",
    "        current_state[\"human_feedback\"] = {}\n",
    "    \n",
    "    current_state[\"human_feedback\"][\"requested\"] = True\n",
    "    current_state[\"human_feedback\"][\"timestamp\"] = \"2025-04-17T10:45:00Z\"  # This would be actual timestamp in production\n",
    "    current_state[\"human_feedback\"][\"comment\"] = response.response\n",
    "    current_state[\"human_feedback\"][\"status\"] = \"completed\"\n",
    "    \n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    return response.response\n",
    "\n",
    "async def process_human_feedback(ctx: Context, approval: bool, feedback: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Processes human feedback on the decision.\n",
    "    \n",
    "    Args:\n",
    "        approval: Whether the human approves the decision\n",
    "        feedback: Optional feedback from the human reviewer\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation of feedback processed\n",
    "    \"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    \n",
    "    # Update the human feedback status\n",
    "    if \"human_feedback\" not in current_state:\n",
    "        current_state[\"human_feedback\"] = {}\n",
    "    \n",
    "    current_state[\"human_feedback\"][\"status\"] = \"completed\"\n",
    "    current_state[\"human_feedback\"][\"approved\"] = approval\n",
    "    current_state[\"human_feedback\"][\"feedback\"] = feedback\n",
    "    current_state[\"human_feedback\"][\"processed_timestamp\"] = \"2025-04-14T10:40:00Z\"  # This would be actual timestamp\n",
    "    \n",
    "    # If not approved, record for model improvement\n",
    "    if not approval:\n",
    "        if \"model_improvement_data\" not in current_state:\n",
    "            current_state[\"model_improvement_data\"] = []\n",
    "        \n",
    "        current_state[\"model_improvement_data\"].append({\n",
    "            \"original_decision\": current_state.get(\"decision\", {}).get(\"outcome\", \"UNKNOWN\"),\n",
    "            \"feedback\": feedback,\n",
    "            \"submission_data\": current_state.get(\"submission_data\", {}),\n",
    "            \"assessment_results\": current_state.get(\"assessment_results\", {})\n",
    "        })\n",
    "    \n",
    "    await ctx.set(\"state\", current_state)\n",
    "    \n",
    "    if approval:\n",
    "        return \"Human feedback processed: Decision approved.\"\n",
    "    else:\n",
    "        return f\"Human feedback processed: Decision requires changes. Feedback: {feedback}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agents with sequential handoff pattern\n",
    "hazard_agent = FunctionAgent(\n",
    "    name=\"HazardClassificationAgent\",\n",
    "    description=\"Evaluates the hazard classification of properties based on building type, construction, and occupancy.\",\n",
    "    system_prompt=(\n",
    "        \"You are a specialist in evaluating property hazard classifications for insurance purposes. \"\n",
    "        \"You analyze building types, construction materials, and occupancy types to determine the risk level. \"\n",
    "        \"Be thorough and precise in your assessments, using standard insurance industry criteria. \"\n",
    "        \"After completing your assessment, hand off to the VulnerabilityAssessmentAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[evaluate_hazard_classification],\n",
    "    can_handoff_to=[\"VulnerabilityAssessmentAgent\"]\n",
    ")\n",
    "\n",
    "vulnerability_agent = FunctionAgent(\n",
    "    name=\"VulnerabilityAssessmentAgent\",\n",
    "    description=\"Evaluates property vulnerability based on security systems and protective measures.\",\n",
    "    system_prompt=(\n",
    "        \"You are a specialist in evaluating property vulnerabilities for insurance purposes. \"\n",
    "        \"You assess security systems, fire protection measures, and other safety features to determine the risk level. \"\n",
    "        \"Be thorough in considering all protective measures and their effectiveness. \"\n",
    "        \"After completing your assessment, hand off to the CATModelingAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[evaluate_vulnerability],\n",
    "    can_handoff_to=[\"CATModelingAgent\"]\n",
    ")\n",
    "\n",
    "cat_modeling_agent = FunctionAgent(\n",
    "    name=\"CATModelingAgent\",\n",
    "    description=\"Evaluates catastrophe risks based on geographical location and environmental factors.\",\n",
    "    system_prompt=(\n",
    "        \"You are a specialist in catastrophe risk modeling for insurance purposes. \"\n",
    "        \"You analyze geographical locations to assess risks from natural disasters like floods, earthquakes, and storms. \"\n",
    "        \"Use precise geographical data and historical patterns to determine risk levels. \"\n",
    "        \"After completing your assessment, hand off to the GuidelinesComplianceAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[evaluate_cat_modeling],\n",
    "    can_handoff_to=[\"GuidelinesComplianceAgent\"]\n",
    ")\n",
    "\n",
    "guidelines_agent = FunctionAgent(\n",
    "    name=\"GuidelinesComplianceAgent\", \n",
    "    description=\"Checks if submissions comply with underwriting guidelines for limits, deductibles, and pricing factors.\",\n",
    "    system_prompt=(\n",
    "        \"You are a specialist in insurance underwriting guidelines compliance. \"\n",
    "        \"You verify that submissions meet all required parameters for coverage limits, deductibles, and other factors. \"\n",
    "        \"Be precise in identifying any deviations from established guidelines. \"\n",
    "        \"After completing your assessment, hand off to the DecisionAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[check_guidelines_compliance],\n",
    "    can_handoff_to=[\"DecisionAgent\"]\n",
    ")\n",
    "\n",
    "decision_agent = FunctionAgent(\n",
    "    name=\"DecisionAgent\",\n",
    "    description=\"Analyzes all assessment results and makes the final underwriting decision.\",\n",
    "    system_prompt=(\n",
    "        \"You are a decision specialist for insurance underwriting. \"\n",
    "        \"You analyze assessment results from multiple domains to determine whether to proceed with quotation, \"\n",
    "        \"recommend a surveyor, or reject a submission. \"\n",
    "        \"First check that all required assessments (hazard, vulnerability, cat modeling, guidelines compliance) \"\n",
    "        \"have been completed before making your decision. \"\n",
    "        \"Balance all risk factors to make the most appropriate decision. \"\n",
    "        \"After making your decision, hand off to the CommunicationAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[make_decision],\n",
    "    can_handoff_to=[\"CommunicationAgent\"]\n",
    ")\n",
    "\n",
    "communication_agent = FunctionAgent(\n",
    "    name=\"CommunicationAgent\",\n",
    "    description=\"Formats and sends notifications based on the underwriting decision.\",\n",
    "    system_prompt=(\n",
    "        \"You are a communication specialist for insurance operations. \"\n",
    "        \"You create clear, appropriate notifications to inform stakeholders about underwriting decisions. \"\n",
    "        \"Ensure all relevant information is included in the appropriate format. \"\n",
    "        \"After sending the notification, the decision process is complete and no further action is needed.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[send_notification]\n",
    ")\n",
    "\n",
    "supervisor_agent = FunctionAgent(\n",
    "    name=\"SupervisorAgent\",\n",
    "    description=\"Orchestrates the overall rank and reject workflow, managing state and decision processes.\",\n",
    "    system_prompt=(\n",
    "        \"You are the supervisor for the insurance submission rank and reject workflow. \"\n",
    "        \"You coordinate multiple specialized agents, maintain the overall state, and ensure the process runs smoothly. \"\n",
    "        \"Delegate tasks to appropriate specialists and synthesize their results.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[],\n",
    "    can_handoff_to=[\"HazardClassificationAgent\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_submission = {\n",
    "    \"submission_id\": \"SUB20250414001\",\n",
    "    \"broker_name\": \"ABC Insurance Brokers\",\n",
    "    \"insured_name\": \"Acme Manufacturing Ltd\",\n",
    "    \"property_details\": {\n",
    "        \"address\": \"123 Industrial Way, Birmingham, B6 4BD\",\n",
    "        \"building_type\": \"Manufacturing\",\n",
    "        \"construction\": \"Steel Frame with Brick\",\n",
    "        \"year_built\": 1995,\n",
    "        \"area_sqm\": 5000,\n",
    "        \"stories\": 2,\n",
    "        \"occupancy\": \"Light Manufacturing - Electronics\",\n",
    "        \"sprinklers\": True,\n",
    "        \"alarm_system\": \"Grade A - 24hr Monitored\"\n",
    "    },\n",
    "    \"coverage\": {\n",
    "        \"building_value\": 4500000.00,\n",
    "        \"contents_value\": 2000000.00,\n",
    "        \"business_interruption\": 3000000.00,\n",
    "        \"deductible\": 25000.00\n",
    "    },\n",
    "    \"compliance_status\": \"PASSED\",\n",
    "    \"client_history\": {\n",
    "        \"claims_count\": 1,\n",
    "        \"previous_policies\": 2,\n",
    "        \"risk_score\": 68\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_insurance_workflow(submission_data=None):\n",
    "    \"\"\"\n",
    "    Create a streamlined workflow for insurance submission processing\n",
    "    \"\"\"\n",
    "    if submission_data is None:\n",
    "        submission_data = example_submission\n",
    "        \n",
    "    # Create the workflow with the 6 essential agents\n",
    "    workflow = AgentWorkflow(\n",
    "        agents=[\n",
    "            hazard_agent,\n",
    "            vulnerability_agent,\n",
    "            cat_modeling_agent,\n",
    "            guidelines_agent,\n",
    "            decision_agent,\n",
    "            communication_agent\n",
    "        ],\n",
    "        # Start with hazard agent\n",
    "        root_agent=hazard_agent.name,\n",
    "        initial_state={\n",
    "            \"submission_data\": submission_data,\n",
    "            \"assessment_results\": {},\n",
    "            # FIXED: Initialize with an empty list instead of a set\n",
    "            \"completed_assessments\": [],\n",
    "            \"decision\": {},\n",
    "            \"notification\": {},\n",
    "            \"workflow_complete\": False\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow(submission_data=None):\n",
    "    \"\"\"Run the multi-agent workflow with human-in-the-loop capability\"\"\"\n",
    "    workflow = create_insurance_workflow(submission_data)\n",
    "    \n",
    "    logger.info(\"Starting workflow execution...\")\n",
    "    \n",
    "    # Run the workflow with a more explicit message\n",
    "    handler = workflow.run(\n",
    "        user_msg=(\n",
    "            \"Please process this insurance submission through the full assessment workflow. \"\n",
    "            \"Begin with the hazard classification, then evaluate vulnerability, \"\n",
    "            \"perform CAT modeling, check guidelines compliance, \"\n",
    "            \"make a decision, and finally send the appropriate notification.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Stream the output to see progress\n",
    "    current_agent = None\n",
    "    \n",
    "    async for event in handler.stream_events():\n",
    "        # Check if workflow is complete after each step\n",
    "        current_state = await handler.ctx.get(\"state\")\n",
    "        if current_state.get(\"workflow_complete\", False):\n",
    "            logger.info(\"Workflow has been marked as complete, stopping further processing.\")\n",
    "            break\n",
    "            \n",
    "        if hasattr(event, \"current_agent_name\") and event.current_agent_name != current_agent:\n",
    "            current_agent = event.current_agent_name\n",
    "            logger.info(f\"==== Agent: {current_agent} ====\")\n",
    "            \n",
    "        # Handle human input requests\n",
    "        if isinstance(event, InputRequiredEvent):\n",
    "            # In a real application, this could be a GUI input, email notification, etc.\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"\\nHUMAN REVIEW REQUIRED\")\n",
    "            print(\"=\"*50)\n",
    "            response = input(event.prefix + \"\\n> \")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "            \n",
    "            # Send the human response back to the workflow\n",
    "            handler.ctx.send_event(\n",
    "                HumanResponseEvent(\n",
    "                    response=response,\n",
    "                    user_name=event.user_name,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Check if a final decision has been made\n",
    "        if current_state.get(\"decision\", {}).get(\"final\", False) and current_agent == \"CommunicationAgent\":\n",
    "            logger.info(\"Final decision has been made and notification sent. Workflow will complete.\")\n",
    "            current_state[\"workflow_complete\"] = True\n",
    "            await handler.ctx.set(\"state\", current_state)\n",
    "    \n",
    "    # Get the final state\n",
    "    final_state = await handler.ctx.get(\"state\")\n",
    "    \n",
    "    # Log the results\n",
    "    logger.info(\"Workflow execution completed.\")\n",
    "    logger.info(f\"Decision: {final_state.get('decision', {}).get('outcome', 'No decision')}\")\n",
    "    logger.info(f\"Reason: {final_state.get('decision', {}).get('reason', 'No reason provided')}\")\n",
    "    \n",
    "    # Return the final state\n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 12:19:13,106 - RankRejectAgent - INFO - Starting the insurance submission assessment system...\n",
      "2025-04-17 12:19:13,108 - RankRejectAgent - INFO - Starting workflow execution...\n",
      "2025-04-17 12:19:13,285 - RankRejectAgent - INFO - ==== Agent: HazardClassificationAgent ====\n",
      "2025-04-17 12:19:14,541 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:14,590 - RankRejectAgent - INFO - Starting hazard classification assessment\n",
      "2025-04-17 12:19:14,591 - RankRejectAgent - INFO - Analyzing property: Manufacturing building with Steel Frame with Brick construction\n",
      "2025-04-17 12:19:14,591 - RankRejectAgent - INFO - Calculated hazard score: 2.5\n",
      "2025-04-17 12:19:14,592 - RankRejectAgent - INFO - Updated completed_assessments: ['hazard']\n",
      "2025-04-17 12:19:14,592 - RankRejectAgent - INFO - Hazard assessment completed and stored in context\n",
      "2025-04-17 12:19:14,599 - RankRejectAgent - INFO - ==== Agent: VulnerabilityAssessmentAgent ====\n",
      "2025-04-17 12:19:14,888 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:15,544 - RankRejectAgent - INFO - Starting vulnerability assessment\n",
      "2025-04-17 12:19:15,545 - RankRejectAgent - INFO - Security features: Sprinklers=True, Alarm=Grade A - 24hr Monitored\n",
      "2025-04-17 12:19:15,545 - RankRejectAgent - INFO - Calculated vulnerability score: 1.0\n",
      "2025-04-17 12:19:15,546 - RankRejectAgent - INFO - Updated completed_assessments: ['hazard', 'vulnerability']\n",
      "2025-04-17 12:19:15,546 - RankRejectAgent - INFO - Vulnerability assessment completed and stored in context\n",
      "2025-04-17 12:19:15,900 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:16,127 - RankRejectAgent - INFO - ==== Agent: CATModelingAgent ====\n",
      "2025-04-17 12:19:16,509 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:17,065 - RankRejectAgent - INFO - Starting CAT modeling assessment\n",
      "2025-04-17 12:19:17,066 - RankRejectAgent - INFO - Analyzing location: 123 Industrial Way, Birmingham, B6 4BD\n",
      "2025-04-17 12:19:17,066 - RankRejectAgent - INFO - Calculated CAT score: 2.0 (flood_risk: 2.0, earthquake_risk: 1.5)\n",
      "2025-04-17 12:19:17,066 - RankRejectAgent - INFO - Updated completed_assessments: ['hazard', 'vulnerability', 'cat_modeling']\n",
      "2025-04-17 12:19:17,067 - RankRejectAgent - INFO - CAT modeling assessment completed and stored in context\n",
      "2025-04-17 12:19:17,467 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:17,797 - RankRejectAgent - INFO - ==== Agent: GuidelinesComplianceAgent ====\n",
      "2025-04-17 12:19:19,263 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:19,592 - RankRejectAgent - INFO - Starting guidelines compliance assessment\n",
      "2025-04-17 12:19:19,592 - RankRejectAgent - INFO - Analyzing coverage: Building value=4500000.0, Deductible=25000.0, Business interruption=3000000.0\n",
      "2025-04-17 12:19:19,593 - RankRejectAgent - INFO - Compliance assessment - Issues: 0, Score: 1.0\n",
      "2025-04-17 12:19:19,593 - RankRejectAgent - INFO - Updated completed_assessments: ['hazard', 'vulnerability', 'cat_modeling', 'guidelines_compliance']\n",
      "2025-04-17 12:19:19,594 - RankRejectAgent - INFO - Guidelines compliance assessment completed and stored in context\n",
      "2025-04-17 12:19:20,095 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:20,306 - RankRejectAgent - INFO - ==== Agent: DecisionAgent ====\n",
      "2025-04-17 12:19:20,620 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:19:20,647 - RankRejectAgent - INFO - Starting decision making process\n",
      "2025-04-17 12:19:20,648 - RankRejectAgent - INFO - All assessments are complete. Proceeding with decision making.\n",
      "2025-04-17 12:19:20,649 - RankRejectAgent - INFO - Assessment scores - Hazard: 2.5, Vulnerability: 1.0, CAT: 2.0, Compliance: 1.0\n",
      "2025-04-17 12:19:20,649 - RankRejectAgent - INFO - Calculated composite risk score: 1.7499999999999998\n",
      "2025-04-17 12:19:20,650 - RankRejectAgent - INFO - Initial decision: PROCEED_TO_QUOTATION, Reason: Risk profile is within acceptable parameters, Confidence: 0.65\n",
      "2025-04-17 12:19:20,650 - RankRejectAgent - INFO - Decision requires human review. Confidence: 0.65\n",
      "2025-04-17 12:19:20,651 - RankRejectAgent - INFO - Requesting human feedback: Please review decision: PROCEED_TO_QUOTATION for submission SUB20250414001. Confidence: 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "HUMAN REVIEW REQUIRED\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 12:20:09,147 - RankRejectAgent - INFO - Human feedback received: \n",
      "2025-04-17 12:20:09,148 - RankRejectAgent - INFO - Decision made and stored in context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 12:20:09,503 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:20:09,661 - RankRejectAgent - INFO - ==== Agent: CommunicationAgent ====\n",
      "2025-04-17 12:20:09,662 - RankRejectAgent - INFO - Final decision has been made and notification sent. Workflow will complete.\n",
      "2025-04-17 12:20:10,010 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 12:20:10,013 - RankRejectAgent - INFO - Workflow has been marked as complete, stopping further processing.\n",
      "2025-04-17 12:20:10,014 - RankRejectAgent - INFO - Workflow execution completed.\n",
      "2025-04-17 12:20:10,014 - RankRejectAgent - INFO - Decision: PROCEED_TO_QUOTATION\n",
      "2025-04-17 12:20:10,015 - RankRejectAgent - INFO - Reason: Risk profile is within acceptable parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== INSURANCE SUBMISSION ASSESSMENT RESULTS ======\n",
      "Submission ID: SUB20250414001\n",
      "Insured: Acme Manufacturing Ltd\n",
      "\n",
      "COMPLETED ASSESSMENTS:\n",
      "- hazard, vulnerability, cat_modeling, guidelines_compliance\n",
      "\n",
      "RISK ASSESSMENT SUMMARY:\n",
      "- Hazard Score: 2.5/5.0\n",
      "- Vulnerability Score: 1.0/5.0\n",
      "- CAT Risk Score: 2.0/5.0\n",
      "- Compliance: Compliant\n",
      "\n",
      "DECISION: PROCEED_TO_QUOTATION\n",
      "Reason: Risk profile is within acceptable parameters\n",
      "Confidence: 65%\n",
      "\n",
      "HUMAN FEEDBACK:\n",
      "Status: completed\n",
      "Comment: \n",
      "\n",
      "===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 12:20:10,038 - RankRejectAgent - INFO - Starting notification preparation\n",
      "2025-04-17 12:20:10,039 - RankRejectAgent - INFO - Preparing notification for decision: PROCEED_TO_QUOTATION\n",
      "2025-04-17 12:20:10,040 - RankRejectAgent - INFO - Email notification prepared for distribution team regarding submission SUB20250414001\n",
      "2025-04-17 12:20:10,436 - httpx - INFO - HTTP Request: POST https://uksouth-openai-shareable.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    logger.info(\"Starting the insurance submission assessment system...\")\n",
    "    \n",
    "    try:\n",
    "        # Run the workflow\n",
    "        final_state = asyncio.run(run_workflow())\n",
    "        \n",
    "        # Print the decision details\n",
    "        decision = final_state.get(\"decision\", {})\n",
    "        \n",
    "        print(\"\\n====== INSURANCE SUBMISSION ASSESSMENT RESULTS ======\")\n",
    "        print(f\"Submission ID: {final_state.get('submission_data', {}).get('submission_id', 'Unknown')}\")\n",
    "        print(f\"Insured: {final_state.get('submission_data', {}).get('insured_name', 'Unknown')}\")\n",
    "        print(\"\\nCOMPLETED ASSESSMENTS:\")\n",
    "        print(f\"- {', '.join(final_state.get('completed_assessments', []))}\")\n",
    "        print(\"\\nRISK ASSESSMENT SUMMARY:\")\n",
    "        \n",
    "        assessment_summary = decision.get(\"assessment_summary\", {})\n",
    "        print(f\"- Hazard Score: {assessment_summary.get('hazard_score', 0):.1f}/5.0\")\n",
    "        print(f\"- Vulnerability Score: {assessment_summary.get('vulnerability_score', 0):.1f}/5.0\")\n",
    "        print(f\"- CAT Risk Score: {assessment_summary.get('cat_score', 0):.1f}/5.0\")\n",
    "        print(f\"- Compliance: {'Compliant' if assessment_summary.get('is_compliant', False) else 'Non-compliant'}\")\n",
    "        \n",
    "        print(f\"\\nDECISION: {decision.get('outcome', 'Unknown')}\")\n",
    "        print(f\"Reason: {decision.get('reason', 'No reason provided')}\")\n",
    "        print(f\"Confidence: {decision.get('confidence', 0):.0%}\")\n",
    "        \n",
    "        # Show human feedback if available\n",
    "        human_feedback = final_state.get(\"human_feedback\", {})\n",
    "        if human_feedback:\n",
    "            print(\"\\nHUMAN FEEDBACK:\")\n",
    "            print(f\"Status: {human_feedback.get('status', 'Not provided')}\")\n",
    "            print(f\"Comment: {human_feedback.get('comment', 'No comment')}\")\n",
    "        \n",
    "        # Show notification summary\n",
    "        notification = final_state.get(\"notification\", {})\n",
    "        if notification:\n",
    "            print(\"\\nNOTIFICATION SENT:\")\n",
    "            print(f\"Recipient: {notification.get('recipient', 'Unknown')}\")\n",
    "            print(f\"Timestamp: {notification.get('timestamp', 'Unknown')}\")\n",
    "            print(\"Email Content Preview: \")\n",
    "            content = notification.get(\"content\", \"\")\n",
    "            print(content[:min(200, len(content))] + \"...\" if len(content) > 200 else content)\n",
    "        \n",
    "        print(\"\\n===========================================\")\n",
    "        \n",
    "        return final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error running workflow: {str(e)}\")\n",
    "        logger.error(f\"Error type: {type(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
